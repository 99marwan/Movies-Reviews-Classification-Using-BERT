{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 AI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/99marwan/Movies-Reviews-Classification-Using-BERT/blob/main/Movies-Reviews-Classification-Using-BERT(ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4 AI"
      ],
      "metadata": {
        "id": "D6jrgaTzEOil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download IMDB Dataset"
      ],
      "metadata": {
        "id": "yRjCUQFMaKYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle==1.5.6\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "! unzip /content/imdb-dataset-of-50k-movie-reviews.zip\n",
        "! rm /content/imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0wkwtrfaOYY",
        "outputId": "3269cd99-ab5c-416d-8446-30cfd16caed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.6\n",
            "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.6) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72858 sha256=985d0c3527e7951939e6fd096dc9207ea68154f85b61f14aa344e84c97ae0b50\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.6\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 35% 9.00M/25.7M [00:00<00:00, 86.6MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 161MB/s] \n",
            "Archive:  /content/imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "KmsDMnvsEhjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# Import Cross Validation\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Import Train_Test_Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scaling\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Import nltk library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#nltk.download('corpus')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize , RegexpTokenizer\n",
        "from string import punctuation\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "wordnet = nltk.corpus.wordnet\n",
        "from nltk import pos_tag\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "#DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} \n",
        "\n",
        "# Scoring\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#model\n",
        "#!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install transformers \n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pfwur6QEkBk",
        "outputId": "d57ca886-e60b-4bd2-f989-85ff467a0964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Balancing"
      ],
      "metadata": {
        "id": "wjUDYJDaEmEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = [\"review\", \"sentiment\"]\n",
        "data = pd.read_csv(\"/content/IMDB Dataset.csv\", names=col_names)\n",
        "#data = pd.read_csv(\"/content/IMDB Dataset.csv\", names=col_names, quoting=csv.QUOTE_NONE)\n",
        "data_positive = data[data[\"sentiment\"] == \"positive\"]\n",
        "data_negative = data[data[\"sentiment\"] == \"negative\"]\n",
        "\n",
        "#balance_data_positive = data_positive.sample(n = len(data_negative), random_state=5)\n",
        "print(data_positive)\n",
        "print(data_negative)\n",
        "#print(balance_data_positive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d36XJKvYEqZ8",
        "outputId": "1e8577d5-24e5-4013-ac39-54de0e0c3b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "1      One of the other reviewers has mentioned that ...  positive\n",
            "2      A wonderful little production. <br /><br />The...  positive\n",
            "3      I thought this was a wonderful way to spend ti...  positive\n",
            "5      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "6      Probably my all-time favorite movie, a story o...  positive\n",
            "...                                                  ...       ...\n",
            "49984  I loved it, having been a fan of the original ...  positive\n",
            "49986  Imaginary Heroes is clearly the best film of t...  positive\n",
            "49990  I got this one a few weeks ago and love it! It...  positive\n",
            "49993  John Garfield plays a Marine who is blinded by...  positive\n",
            "49996  I thought this movie did a down right good job...  positive\n",
            "\n",
            "[25000 rows x 2 columns]\n",
            "                                                  review sentiment\n",
            "4      Basically there's a family where a little boy ...  negative\n",
            "8      This show was an amazing, fresh & innovative i...  negative\n",
            "9      Encouraged by the positive comments about this...  negative\n",
            "11     Phil the Alien is one of those quirky films wh...  negative\n",
            "12     I saw this movie when I was about 12 when it c...  negative\n",
            "...                                                  ...       ...\n",
            "49995  This is your typical junk comedy.<br /><br />T...  negative\n",
            "49997  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49998  I am a Catholic taught in parochial elementary...  negative\n",
            "49999  I'm going to have to disagree with the previou...  negative\n",
            "50000  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[25000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Split"
      ],
      "metadata": {
        "id": "4ckQ9DAnEqub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratios\n",
        "ratio_train = 0.70\n",
        "ratio_val = 0.10\n",
        "ratio_test = 0.20\n",
        "\n",
        "# Data to train models\n",
        "final_data = np.concatenate((data_positive,data_negative),axis=0)\n",
        "X_final_data = final_data[:, :-1]\n",
        "y_final_data = final_data[: , 1]\n",
        "\n",
        "# Produce test set (20%)\n",
        "X_remaining, X_test, y_remaining, y_test = train_test_split(X_final_data, y_final_data, test_size=ratio_test, random_state=5, stratify=y_final_data)\n",
        "\n",
        "# Adjust ratios so that validation set is 10% of original data size (0.125 of remaining = 0.10 of original)\n",
        "ratio_remaining = 1 - ratio_test\n",
        "ratio_val_adjusted = ratio_val / ratio_remaining\n",
        "\n",
        "# Produce train set (70%) and validation set (10%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_remaining, y_remaining, test_size=ratio_val_adjusted, random_state=5, stratify=y_remaining)\n",
        "\n",
        "# Printing checks\n",
        "print(len(final_data))\n",
        "print(final_data)\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(X_train))\n",
        "print(X_train)\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(y_train))\n",
        "print(y_train)\n",
        "print(np.count_nonzero(y_train == \"positive\"))\n",
        "print(np.count_nonzero(y_train == \"negative\"))\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(X_val))\n",
        "print(X_val)\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(y_val))\n",
        "print(y_val)\n",
        "print(np.count_nonzero(y_val == \"positive\"))\n",
        "print(np.count_nonzero(y_val == \"negative\"))\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(X_test))\n",
        "print(X_test)\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "print(len(y_test))\n",
        "print(y_test)\n",
        "print(np.count_nonzero(y_test == \"positive\"))\n",
        "print(np.count_nonzero(y_test == \"negative\"))\n",
        "\n",
        "print(\"==============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvLIxGlYEszc",
        "outputId": "08f5945e-e7f6-4104-d55a-faf314e53db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "[[\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"\n",
            "  'positive']\n",
            " ['A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'\n",
            "  'positive']\n",
            " ['I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'\n",
            "  'positive']\n",
            " ...\n",
            " ['I am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school & college. I am still a practicing Catholic but would not be considered a \"good Catholic\" in the church\\'s eyes because I don\\'t believe certain things or act certain ways just because the church tells me to.<br /><br />So back to the movie...its bad because two people are killed by this nun who is supposed to be a satire as the embodiment of a female religious figurehead. There is no comedy in that and the satire is not done well by the over acting of Diane Keaton. I never saw the play but if it was very different from this movies then it may be good.<br /><br />At first I thought the gun might be a fake and the first shooting all a plan by the female lead of the four former students as an attempt to demonstrate Sister Mary\\'s emotional and intellectual bigotry of faith. But it turns out the bullets were real and the story has tragedy...the tragedy of loss of life (besides the two former students...the lives of the aborted babies, the life of the student\\'s mom), the tragedy of dogmatic authority over love of people, the tragedy of organized religion replacing true faith in God. This is what is wrong with today\\'s Islam, and yesterday\\'s Judaism and Christianity.'\n",
            "  'negative']\n",
            " ['I\\'m going to have to disagree with the previous comment and side with Maltin on this one. This is a second rate, excessively vicious Western that creaks and groans trying to put across its central theme of the Wild West being tamed and kicked aside by the steady march of time. It would like to be in the tradition of \"Butch Cassidy and the Sundance Kid\", but lacks that film\\'s poignancy and charm. Andrew McLaglen\\'s direction is limp, and the final 30 minutes or so are a real botch, with some incomprehensible strategy on the part of heroes Charlton Heston and Chris Mitchum. (Someone give me a holler if you can explain to me why they set that hillside on fire.) There was something callous about the whole treatment of the rape scene, and the woman\\'s reaction afterwards certainly did not ring true. Coburn is plenty nasty as the half breed escaped convict out for revenge, but all of his fellow escapees are underdeveloped (they\\'re like bowling pins to be knocked down one by one as the story lurches forward). Michael Parks gives one of his typically shifty, lethargic, mumbling performances, but in this case it was appropriate as his modern style sheriff symbolizes the complacency that technological progress can bring about.'\n",
            "  'negative']\n",
            " [\"No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some of the best episodes. Unfortunately, this movie had a muddled, implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scenes with Kirk, Spock and McCoy at Yosemite.<br /><br />I would say this movie is not worth a rental, and hardly worth watching, however for the True Fan who needs to see all the movies, renting this movie is about the only way you'll see it - even the cable channels avoid this movie.\"\n",
            "  'negative']]\n",
            "==============================\n",
            "35000\n",
            "[[\"I really like this show. It has drama, romance, and comedy all rolled into one. I am 28 and I am a married mother, so I can identify both with Lorelei's and Rory's experiences in the show. I have been watching mostly the repeats on the Family Channel lately, so I am not up-to-date on what is going on now. I think females would like this show more than males, but I know some men out there would enjoy it! I really like that is an hour long and not a half hour, as th hour seems to fly by when I am watching it! Give it a chance if you have never seen the show! I think Lorelei and Luke are my favorite characters on the show though, mainly because of the way they are with one another. How could you not see something was there (or take that long to see it I guess I should say)? <br /><br />Happy viewing!\"]\n",
            " [\"Ed Wood is eclipsed and becomes Orson Welles. This film is fantastic. Vampire witches who fight in terribly choreographed scenes and dialog that could have breaking ribs with laughter. Plan 9 From OUterSpace dons't stand a chance against this. Described by the writer and psychic Stephen Armourae on the Vampire Forum as a masterpiece- he's from England and thoroughly sarcastic.<br /><br />It has Stephanie Beaton and the producers know whats going to save them from bankcrupcy by repeatedly using her. Though she leaves me cold as she looks more like the undead than all the devil raisers. And Eileen Daly is just a lower rate Elvira. The whole thing is badly done.<br /><br />Watch it for the script though\"]\n",
            " ['Yes, this movie is a real thief. It stole some shiny Oscars from Avatar just because politicians wanted another war-hero movie to boost the acceptance (support?) for the wars U.S. is still fighting today. I do not really want to go here into politics, but come on, this is more clear than the summer sky. Hurt locker does not really have anything outstanding, no real plot at all. I really feel myself in the 50\\'s of Hungary when the party told the people what to like and what not to like. The same propaganda movies were produced that time, only with the exception that those were black and white. Even if we consider this title a reasonable piece of the \"U.S. wars are cool\" genre, you surely have much better movies to choose from.']\n",
            " ...\n",
            " [\"Don't even waste your time, let alone pay rental for this piece of dreck! How it got made is beyond me. (I don't know why there's a minimum of 10 lines... I've already summarized this trashy movie, but, oh well...) The acting was awful, like they all needed lessons. The plot was weak, the ending... Feh! I think the cinematography was the only thing that didn't totally suck... well, maybe the sound was minimalistically OK. The one good thing is, if they could make this movie, even make some money with it, there may be hope for any screenwriter with a REAL idea. So, you-all take heart! I guess the same holds true of actors... if these people actually got paid, then you can, too!\"]\n",
            " [\"There are good ways to make a movie and bad ways and this very much the former. This short caper exacts nothing more than what it gives to the audience. It presents a simple story, told very plainly with enough wisecracks to keep you going, then just gets better and better. Clooney's cameo is funny and very welcome but the leads including Sam Rockwell and Luiz Guzman can easily make it on their own. Likeable and funny, hilariously so towards the end, Welcome to Collinwood is a welcome addition to the heist genre.\"]\n",
            " ['This movie is definitely on the list of my top 10 favorites. The voices for the animals are wonderful. Sally Field and Michael J. Fox are both brilliant as the sassy feline and the young inexperienced pooch, but the real standout is Don Ameche as the old, faithful golden retriever. This movie is a great family movie because it can be appreciated and loved by children as well as adults. Humorous and suspenseful, and guaranteed to make every animal lover cry! (happy tears!)']]\n",
            "==============================\n",
            "35000\n",
            "['positive' 'negative' 'negative' ... 'negative' 'positive' 'positive']\n",
            "17500\n",
            "17500\n",
            "==============================\n",
            "5000\n",
            "[[\"It's really too bad that nobody knows about this movie. I think if it were just spruced up a little and if it weren't so low-budget, I think one of the major film companies might have wanted to take it. I first saw this movie when I was 11, and I thought it was so powerful with the many great, yet illegal lengths that Mitchell goes to just to keep his family together. It inspired me then and it amazes me now. If you're lucky enough to find a copy of this movie, don't miss it!\"]\n",
            " ['Loki, Norse god of mischief, creates a mask that endows the wearer with cartoon-like powers. At the command of his father, Odin, he spends the rest of the movie looking for the mask so that it can cause no further grief to mankind. In the meantime, the possessor of the mask conceives a child who inherits the powers of the mask. Etc. etc. If this sounds like a pretty thin plot line, it is. Add to this the fact that the movie is handled ineptly from start to finish, and the result is very, very bad. You can find worse movies, but you\\'ll have to actively search for them.<br /><br />For the most part, Son of the Mask is presented at the intellectual level of a pre-schooler, but in light of scenes such as the mask-baby urinating copiously in six different directions, including on his father, this premise seems unlikely. I asked my son who he thought might have been the target audience for the movie, and he responded \"Convicted felons,\" apparently forgetting for the moment that the Constitution prohibits cruel and unusual punishment.<br /><br />But just making a bad movie is not a sin, or Hell would be overflowing. What makes it a sin is that $72 million was spent on this piece of garbage. To put things in perspective, the day after we watched Son of the Mask, my son and I watched \"Good Night, and Good Luck,\" a movie that garnered six academy award nominations (including best picture), and was brought in for $7 million. That\\'s right. Just one-tenth of the amount of money spent on Son of the Mask. This, then is the sin -- to flush good money down the sewer, when it could have been better used in making watchable movies, or feeding starving children, or for that matter, almost any other purpose. The producers should truly be ashamed of themselves.']\n",
            " ['Dissapointing action movie with an interesting premise: a young Mafia would-to-be killer (Chandler) must demonstrate to his boss that he is a good man for the service so he goes to California to take some lessons with a very known professional killer (Beluschi). First and most important task: to kill a young woman (Lee) that is a completely strange for all of them. But is she a easy target? The movie goes on and on based upon this principal idea but the result is just bad routine; even the weird twist at the end does not save the movie. Good performance by Chandler. I give this a 4 (four).']\n",
            " ...\n",
            " [\"This film is just another waste of time. The plot is ridiculous, forced USA drama. The characters were all really weak, especially the uncharismatic Goya and the bad interpretation of Bardem, who only was alright in his classic interpretation, when acting as french ally.<br /><br />Just another chance lost of have spent the money in a good film. I guess it was no a low budget film. Definitely not recommended. Maybe the director's should think a bit whether the film has sense or not before wasting so that money. Maybe they do not bother as they have profits before launching them in the cinema.<br /><br />No more hope in cinema...\"]\n",
            " ['Still a sucker for Pyun\\'s esthetic sense, I liked this movie, though the \"unfinished\" ending was a let-down. As usual, Pyun develops a warped sense of humour and Kathy Long\\'s fights are extremely impressive. Beautifully photographed, this has the feel it was done for the big screen.']\n",
            " [\"Ultimately too silly and pointless. Yes there is the gilded cage metaphor but probably most kids would miss that. Forgettable. Instantly.<br /><br />Animation is, as we have come to expect, super-real. The plot-line could best be described as thin but tenacious. Although the ending seemed arbitrary to me.<br /><br />The sewer underworld is a suitably disgusting reflection of the world above and, somehow, wealth and money seem to count for a lot there too. Oh yes, and there's a romantic interest with the female being the smarter, more savvy and go-getting of the pair - this in itself is rapidly becoming a tiresome (anti) stereotype. Probably your kids will love it though.\"]]\n",
            "==============================\n",
            "5000\n",
            "['positive' 'negative' 'negative' ... 'negative' 'negative' 'negative']\n",
            "2500\n",
            "2500\n",
            "==============================\n",
            "10000\n",
            "[[\"But I got over it. To me, it seemed that even the Author of the book favored Caroline. I felt so sorry for the character Louise, and she was constantly compared with Esau who was evil, I just felt the comparison was a bit harsh and un-realistic. Really though, the movie was bad. I wouldn't really see it unless you're ready for a big let down.\"]\n",
            " ['People who thought that THE CHRONICLES OF RIDDICK sucked harder than the black hole that swallowed up EVENT HORIZON, probably didn\\'t see the movie that spawned Vin Diesel\\'s skin-headed killer to begin with, and probably have no intention of doing so. Too bad, because PITCH BLACK actually does kick major ass.<br /><br />Directed by genre vet David Twohy, (WARLOCK and the excellent but underrated BELOW) and written by siblings Jim and Ken Wheat (LIES, SILENT SCREAM), PITCH BLACK begins with an \\'ALIEN\\'-esque prologue. When a combination cargo freighter/passenger ship is badly damaged by a freak meteor shower (in which the captain is also killed), the co-pilot, Carolyn Fry (SILENT HILL\\'S Radha Mitchell) has an important decision to make: ditch the cargo or the passengers? Close to picking ore over occupants, the crash landing derails her ultimate course of action.<br /><br />No matter, because the catastrophic landing has been made on a foreboding rock that once held a mining colony. Amongst the survivors are a couple of settlers, Shazza (FARSCAPE\\'s Claudia Black) and Zeke (John Moore); an Imam (Keith David) and his young followers (can you say \"red shirts\" boys and girls?); an antiques dealer named Paris Ogilvie (Lewis Fitz-Gerald); a \\'young boy\\' named Jack (Rhianna Griffith), and the most controversial members of the group: a Marshall named Johns (the excellently slimy Cole Hauser) and his prisoner...a dangerous murderer named Riddick (Diesel). <br /><br />How the group dynamics shake out make for a lot of the dramatic tension, especially with concerns about Riddick and how many people he might slice and dice if he ever gets away. But no one here gets out alive, as the saying goes, and the biggest twists have less to do with how they get along, than how they\\'ll survive when they discover the unthinkable. They are not alone on the planetoid. Things that are hungry, taloned and quick are slithering around just where they can\\'t be seen, living in the darkness where they can survive and thrive. They want the flesh of the new arrivals to sate their appetites, but they can\\'t come out into the searing daylight to forage for food.<br /><br />Does the phrase \"total eclipse\" make things a little more interesting? You betcha. Hence the more-than-fitting title.<br /><br />Vin, more monosyllabic than Clint Eastwood\\'s Dirty Harry on his grumpiest day, dripping more testosterone than sweat, has a field day here with a character that really does seem worth a sequel or two, and Hauser, oozing menace and bile is every bit as good as his dad Wings was at on-screen villainy. Surprisingly, though, Mitchell holds her own and manages to be strong and sympathetic as Fry. You would expect no less than a strong showing from Black, She Who Once Was \\'Aeryn Sun\\', and she doesn\\'t disappoint. (Too bad her role wasn\\'t bigger, but that\\'s all I\\'ll say about it.) And David makes his usual indelible impression as the holy man whose faith will truly be tested in the very pit of Hell itself.<br /><br />The pace moves faster than Riddick slipping up behind his prey with a shank in his teeth, and once the darkness descends, the terror and tension never let up, pretty much as in other classic sci-fi/horror flicks which this imitates. But if imitation truly is the sincerest form of flattery, Dave Twohy and the Wheats deserve major backslaps for getting this one right with a vengeance, and for giving us an ending that is anti-Hollywood to the max.<br /><br />I don\\'t want to spoil the surprises, so I won\\'t say much more, except that if you saw CHRONICLES first and weren\\'t too happy, give PITCH BLACK a chance anyway. And if you\\'ve seen neither, definitely start with this one. <br /><br />I don\\'t know what Vin is up to now, but he could certainly do worse than to give these guys a call again. I\\'d love to see what they would dream up next...']\n",
            " [\"This 1955 heist film follows Tony le Stephanois, recently released from prison for theft, as he undertakes the robbery of his life. He teams up with his old heist buddies and they bring in an expert safe-cracker, Cesar from Milan (played by Jules Dassin, who also directed. He only directed because he had been blacklisted as a communist in the U.S. and couldn't work in Hollywood.) The brilliance of this film is the 1/2 hour during the robbery. During all this time, there is no dialogue and no music, only the muted sounds of digging through the floor or drilling the safe. This increases the suspense and draws you in. They get away with several hundred million francs worth of jewels, but a jewel offered to a dancer by Cesar brings their haul to the attention of a trio of brutal brothers. They set out to get the stash for themselves and bring misfortune in their wake. Great heist/gangster movie, but I prefer J.-P. Melville's films in this genre.<br /><br />This movie is like some lemonade I had last night. I had gone to a Caribbean restaurant and the lemonade was made with sugar cane juice instead of sugar. It also had a lot of ice and was heavy on the lemons, leaving it fairly sour (which I like). The sugar cane juice imparted a subtle, slightly more mellow taste to it than actual sugar, and the ice made sure it was cold and refreshing as I sucked it down. 7.5/10 http://blog.myspace.com/locoformovies\"]\n",
            " ...\n",
            " ['Michelle Pfeiffer stars as a mob widow who seeks a normal life but has her hands full with the new boss and an undercover agent. A lighthearted Demme film with some good laughs and Pfeiffer in a comical role that she has fun with..on a scale of one to ten..8']\n",
            " ['\"Journey to the Far Side of the Sun\" (aka \"Doppelganger\") is an entertaining, Twilight Zone-style sci-fi offering from Gerry and Sylvia Anderson (the team behind Space: 1999, UFO, Thunderbirds, Fireball XL-5 and others). In the film, Roy Thinnes (of the \"Invaders\" TV show) and Ian Hendry star as astronauts sent on a flight to a planet which shares an exact orbit of the earth, but on the opposite side of the sun; hence previously hidden from view. A pushy European space flight director (over-acted by the late Patrick Wymark) gets the flight fast-tracked and after rigorous training , the astronauts are good to go. Thus begins the best sequences in the film, the launch, flight and landing on the \\'other\\' earth. Dazzling rocket miniature work (by Derek Meddings) and a dream-like, elegant spaceflight (somewhat reminiscent of the best moments of \"2001: A Space Odyssey\") are easy highlights of the movie. The landing on the \"doppelganger\" earth is both exciting and eerie. After this, the Twilight Zone aspect of the film kicks in; with a plot lifted almost whole from the classic TZ episode, \"The Parallel.\" That aside, the film is still solid sci-fi, with some intriguing \\'mirror-world\\' stuff to chew on (backwards writing and left-handed handshakes, for example). Less successful are the scenes depicting a mid-21st century earth; where all the men wear turtlenecks and Nehru jackets and all the women wear mini-skirts. Some of the relationships with women in the film are very \\'non-PC\\' by today\\'s standards as well. And (in the most consistent failing of most 20th century sci-fi) the computers, telephones and other hardware are all big, colorful and clunky (right out of Patrick McGoohan\\'s \"The Prisoner\"). No one foresaw the digital microprocessor age! If one can accept these failings in foresight, the movie is very interesting, with a solid lead performance by Thinnes as the troubled astronaut. And with a nice, 1960s/early \\'70s style nihilistic ending! For fans of retro sci-fi (like myself) this is a \"Journey\" worth taking!']\n",
            " [\"'One-Round' Jack Sander is called that because he's a carnival boxer who fights any man in the audience. If they can last one round, they win a prize--a popular way to draw customers into traveling shows long ago. Jack is in love with the ticket girl, Mabel, though her head is quickly turned when Bob Corby enters the ring to try his chances with Jack. What no one at the fight knows is that Bob is the champ, so he's able to beat Jack--though it takes him some work. As a result, Bob asks Jack to become his sparring partner and give up the carnival circuit. Later, Jack improves so much that he, too, becomes a legitimate boxer. Slowly, he works his way up the rankings until he's nearly ready to take on the Champ.<br /><br />In the meantime, the Champ and Mabel start running around behind Jack's back--even though by now Mabel has married Jack. So, when the final fight occurs between Jack and Bob, it's very personal and Jack is ready to kill him. Is he good enough? Will rise justifiable rage against Bob help or hinder his performance? Tune in and see.<br /><br />This film was directed by Alfred Hitchcock and while today this sort of film seems strange for a director known for mystery-suspense films, back in the 1920s, Hitchcock had no fixed genre which he directed or wrote (he did both for this film). In fact, in many ways this film is more indicative of Hitchcock's silent style, as a somewhat similar plot came up in one of his next silents, THE MANXMAN (also starring Carl Brisson as the wronged husband). So, while this seems a lot like a standard boxing film of the day, it was not a radical departure for this great director--even with its rather formulaic ending.<br /><br />Overall, while a bit predictable and having Ian Hunter playing a boxing champ seems silly, the film works well. While far from a perfect silent, it's well worth seeing and packs a nice punch.\"]]\n",
            "==============================\n",
            "10000\n",
            "['negative' 'positive' 'positive' ... 'positive' 'positive' 'positive']\n",
            "5000\n",
            "5000\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Pre-processing"
      ],
      "metadata": {
        "id": "Z7ulQfL6Eyez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_wordnet_pos(tag):\n",
        "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "#     # tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "#     tag_dict = {\"J\": wordnet.ADJ,\n",
        "#                 \"N\": wordnet.NOUN,\n",
        "#                 \"V\": wordnet.VERB,\n",
        "#                 \"R\": wordnet.ADV}\n",
        "\n",
        "#     return tag_dict.get(tag, wordnet.NOUN)\n",
        "def get_wordnet_pos(wordTag):\n",
        "    if wordTag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif wordTag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif wordTag.startswith('N'):\n",
        "        return wordnet.NOUN \n",
        "    elif wordTag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "DB-ORGlOzOKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Lemmatization(sentence):\n",
        "  \n",
        "  lemmatized_Sentence = [lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1])) for word in pos_tag(sentence)]\n",
        "  # lemmatized_Sentence = [lemmatizer.lemmatize(c,get_wordnet_pos(c)) for c in sentence ]\n",
        "  return lemmatized_Sentence"
      ],
      "metadata": {
        "id": "__Qvw6odrHpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Lower_Case(sentence):\n",
        "  lowerCase_Sentences = [c.lower() for c in sentence ]\n",
        "  return lowerCase_Sentences"
      ],
      "metadata": {
        "id": "IqOo6g8VpgbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_StopWords(sentence):\n",
        "  sentence_nonStopWords = [c for c in sentence if c not in stopwords]\n",
        "  return sentence_nonStopWords\n"
      ],
      "metadata": {
        "id": "H5asvqDUbxQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NLTK_Preprocessing(li_quotes):\n",
        "  \n",
        "  #TOKENIZE AND REMOVE PUNCTUATION\n",
        "  tokenized = []\n",
        "  \n",
        "  tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "  for sentence in  li_quotes :\n",
        "      tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "      tokenized_sentence = remove_StopWords(tokenized_sentence)\n",
        "      tokenized_sentence = Lower_Case(tokenized_sentence)\n",
        "      tokenized_sentence = Lemmatization(tokenized_sentence)\n",
        "      tokenized_sentence = \" \".join(tokenized_sentence) \n",
        "    #  tokenized_sentence = word_tokenize(sentence)    \n",
        "      tokenized.append(tokenized_sentence)\n",
        "  \n",
        "\n",
        "  #REMOVE STOPWORDS\n",
        "  #tokenized = list(map(remove_StopWords, tokenized))\n",
        " \n",
        "  #TO LOWER CASE\n",
        "  #tokenized = list(map(Lower_Case, tokenized))\n",
        "\n",
        "  #print(tokenized[0])\n",
        "  #print(type(tokenized[0]))\n",
        "\n",
        "  #print(tokenized[1][4])\n",
        "  #print(get_wordnet_pos(tokenized[1][4]))\n",
        "  #print(tokenized[2])\n",
        "  #tokenized = list(map(Lemmatization, tokenized))\n",
        "\n",
        "  \n",
        "  #print(tokenized[0])\n",
        "  #print(tokenized[1][4])\n",
        "  #print(get_wordnet_pos(tokenized[1][4]))\n",
        "  #print(nltk.pos_tag([tokenized[1][4]]))\n",
        "  #print(get_wordnet_pos_treebank( nltk.pos_tag(tokenized[1][4])[0] ))\n",
        "  #print(tokenized[2])\n",
        "  #sentences = [word_tokenize(string) for string in li_quotes[0]]\n",
        "  #print(sentences)\n",
        "  #tokenized_word=word_tokenize(li_quotes)\n",
        "  #print(tokenized_word)\n",
        "\n",
        "  return tokenized"
      ],
      "metadata": {
        "id": "Y2IR-nV9FOBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing steps\n",
        "# (a) Remove punctuation\n",
        "# (b) Remove stop words\n",
        "# (c) Lowercase all characters\n",
        "# (d) Lemmatization of words\n",
        "\n",
        "# Preparation\n",
        "\n",
        "#punctuation = list(punctuation)\n",
        "\n",
        "\n",
        "\n",
        "# X_train_modified = NLTK_Preprocessing(X_train.ravel())\n",
        "# X_val_modified = NLTK_Preprocessing(X_val.ravel())\n",
        "# X_test_modified = NLTK_Preprocessing(X_test.ravel())\n",
        "# print(nltk.pos_tag([\"amaze\"]))\n",
        "# c = \"amaaaaaaaaaaaaaaaaaazing\"\n",
        "# print(lemmatizer.lemmatize(c,get_wordnet_pos(c)))\n",
        "\n",
        "# print(X_val_modified)\n",
        "\n",
        "\n",
        "#X_train = nltk.word_tokenize(X_train)\n",
        "#cleaned_tokens = [token for token in X_train if token not in stop_words and token not in punctuation]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmvz9FewE149",
        "outputId": "99de44c4-5743-47c4-ace7-cdeaebcdf201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset class\n"
      ],
      "metadata": {
        "id": "kjgYj6TQ0lXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'negative':0,\n",
        "          'positive':1,\n",
        "          }\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, X, Y):\n",
        "\n",
        "        self.labels = [labels[label] for label in Y]\n",
        "      \n",
        "        self.texts = [tokenizer(str(text[0]), \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in X]\n",
        "        \n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "gi6Urrzf0n0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build model\n"
      ],
      "metadata": {
        "id": "ToPfhuOahZYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(768, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(256, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.linear4 = nn.Linear(128, 64)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.linear5 = nn.Linear(64, 1)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear1_output = self.linear1(dropout_output)\n",
        "        relu1 = self.relu1(linear1_output)\n",
        "  \n",
        "        linear2_output = self.linear2(relu1)\n",
        "        relu2 = self.relu2(linear2_output)\n",
        "    \n",
        "        linear3_output = self.linear3(relu2)\n",
        "        relu3 = self.relu3(linear3_output)\n",
        "        \n",
        "        linear4_output = self.linear4(relu3)\n",
        "        relu4 = self.relu4(linear4_output)\n",
        "       \n",
        "        linear5_output = self.linear5(relu4)\n",
        "        # sigmoid = self.sigmoid(linear5_output)\n",
        "\n",
        "        return linear5_output"
      ],
      "metadata": {
        "id": "Ip1-jBHYhYmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train Loop\n",
        "### we need to save model after each epoch ***enter path to save model in it***"
      ],
      "metadata": {
        "id": "qMCGCXhf1PeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train_data, Y_train_data, X_val_data, Y_val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset( X_train_data, Y_train_data), Dataset( X_val_data, Y_val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                #edit\n",
        "                #we need output to be float\n",
        "                train_label = train_label.to(torch.float32)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                #edit\n",
        "                #unsqueeze makes size correct\n",
        "                batch_loss = criterion(output, train_label.unsqueeze(1))\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                output = torch.round(torch.sigmoid(output))\n",
        "                acc = (output == train_label).sum().item()\n",
        "                \n",
        "                #acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "              \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "\n",
        "                    val_label = val_label.to(torch.float32)\n",
        "\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    #batch_loss = criterion(output, val_label)\n",
        "                    batch_loss = criterion(output, val_label.unsqueeze(1))\n",
        "\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    output = torch.round(torch.sigmoid(output))\n",
        "                    acc = (output == val_label).sum().item()\n",
        "\n",
        "                    #acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc     \n",
        "  \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(Y_train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(Y_train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(Y_val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(Y_val_data): .3f}')\n",
        "            \n",
        "            for param_group in optimizer.param_groups:\n",
        "              print(param_group['lr'])\n",
        "            \n",
        "            scheduler.step(total_loss_val)\n",
        "            \n",
        "            #enter path here\n",
        "            torch.save(model, '/content/gdrive/MyDrive/BERT folder/model_prepocessed_1e-6.pt')"
      ],
      "metadata": {
        "id": "891yhu5y1RmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google drive setup to save model\n"
      ],
      "metadata": {
        "id": "G0Le9-GYUnFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW-0rxroUql1",
        "outputId": "5de3d8dd-b3ee-4931-dc02-85a8a579d380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using BERT"
      ],
      "metadata": {
        "id": "gpM23ZnuE2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "\n",
        "LR = 1e-6\n",
        "              \n",
        "# X_train_n, X_v, y_train_n, y_v = train_test_split(X_train, y_train, test_size=0.99, random_state=5, stratify=y_train)\n",
        "# X_val_n, X_z, y_val_n, y_z = train_test_split(X_val, y_val, test_size=0.99, random_state=5, stratify=y_val)\n",
        "\n",
        "X_train_modified = NLTK_Preprocessing(X_train.ravel())\n",
        "\n",
        "              \n",
        "train(model, X_train_modified, y_train, X_val, y_val, LR, EPOCHS)\n"
      ],
      "metadata": {
        "id": "6P205iXTFAxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n"
      ],
      "metadata": {
        "id": "Ptus9V0mEsJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X_test_data, y_test_data):\n",
        "\n",
        "    test = Dataset( X_test_data, y_test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              #edit\n",
        "              #we need output to be float\n",
        "              test_label = test_label.to(torch.float32)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              #acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "\n",
        "              output = torch.round(torch.sigmoid(output))\n",
        "              acc = (output == test_label).sum().item()\n",
        "\n",
        "              total_acc_test += acc\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(y_test_data): .3f}')"
      ],
      "metadata": {
        "id": "E8rnbiuDEwpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X_test_data, y_test_data):\n",
        "\n",
        "    test = Dataset( X_test_data, y_test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "        predY=[]\n",
        "        predY=torch.tensor(predY)\n",
        "        predY=predY.to(device)\n",
        "    \n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              #edit\n",
        "              #we need output to be float\n",
        "              test_label = test_label.to(torch.float32)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              #acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "\n",
        "              output = torch.round(torch.sigmoid(output))\n",
        "              predY=torch.cat((predY,output))\n",
        "              acc = (output == test_label).sum().item()\n",
        "\n",
        "              total_acc_test += acc\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(y_test_data): .3f}')\n",
        "\n",
        "    predY = torch.flatten(predY)\n",
        "    predY=predY.detach().cpu().numpy()\n",
        "    testing=np.where(y_test_data=='positive',1,0)\n",
        "    predY=np.where(predY==1,1,0)\n",
        "    print(predY)\n",
        "    print(testing)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(testing, predY,labels=[0,1]))\n",
        "    print(\"Accuracy:\")\n",
        "    print((accuracy_score(testing, predY) * 100), \"%\")\n",
        "    print(classification_report(testing, predY))\n",
        "\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # plot_confusion_matrix(model(input_id, mask), X_test_data, y_test_data,labels=[0,1])\n",
        "    # plt.show()\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(testing, predY,labels=[0,1]),\n",
        "                              display_labels=[0,1])\n",
        "\n",
        "\n",
        "# NOTE: Fill all variables here with default values of the plot_confusion_matrix\n",
        "    disp = disp.plot()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "79gtZANYR3bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/gdrive/MyDrive/BERT folder/model_unprepocessed_1e-6.pt')\n",
        "\n",
        "#X_test_n, X_v, y_test_n, y_v = train_test_split(X_test, y_test, test_size=0.98, random_state=5, stratify=y_test)\n",
        "evaluate(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "DxD86pLQaaCD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "405e9eca-ca64-4229-aeb6-99019b90694c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.925\n",
            "[0 1 1 ... 1 1 0]\n",
            "[0 1 1 ... 1 1 1]\n",
            "Confusion Matrix:\n",
            "[[4546  454]\n",
            " [ 294 4706]]\n",
            "Accuracy:\n",
            "92.52 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.92      5000\n",
            "           1       0.91      0.94      0.93      5000\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhdVZnv8e8vRQYykREMEEiUAIZRxDDoRQQaAvZt8LYDYEsa6EYUFEVogdYGg1xRlLS0gjKEQVoQcYoYCWmQyyCYkBimQCAmTElIIBUCGUhSVe/9Y68KlVB16uzKOalTtX+f59lPzll7WjuVerPWXnuvVxGBmVnR9OjsCpiZdQYHPzMrJAc/MyskBz8zKyQHPzMrpG06uwItDRnSI0aOrKkqWTteeGpgZ1fBcljbtIr18ba25BjHfKxfLK9vLGvbWU+smxYR47fkfNVSU5Fm5MhtmDZ1WGdXw3KYMLYm/11bGx5dNWWLj7G8vpEZ03Ypa9u6Ec/X7C90TQU/M6t9ATTR1NnV2GIOfmaWSxBsiPK6vbXMwc/McnPLz8wKJwgau8FrsQ5+ZpZbE10/+Pk5PzPLJYBGoqylHJLqJP1V0l3p+02SFkqak5b9U7kkXSVpvqQnJB3Q4hgTJD2flgnlnNctPzPLrcItv3OAZ4CWD42eHxF3brbdscCYtBwEXAMcJGkIcDFwIFlsniVpSkSsKHVSt/zMLJcANkSUtbRH0s7Ax4Hryzj18cAtkXkUGCRpBHAMMD0i6lPAmw60+wCqg5+Z5RJldnnL7Pb+J/Bv8K7h48tS13aSpN6pbCfg5RbbvJLK2iovycHPzPIJaCxzAYZJeqzFckbzYST9PbAsImZtdoYLgT2BDwFDgK9X4zJ8z8/Mcsne8Cjb6xFxYBvrPgz8g6TjgD7AQEm3RsQ/pfXrJN0InJe+LwJGtth/51S2CDh8s/L726uYW35mlpNoLHMpJSIujIidI2IUcCJwX0T8U7qPhyQBJwBPpV2mAKekUd+DgZURsQSYBhwtabCkwcDRqawkt/zMLJdswGOLJoZpz39LGg4ImAOcmcqnAscB84E1wKkAEVEv6VJgZtpuYkTUt3cSBz8zyyV7zq+ywS8i7id1VSPiiDa2CeCsNtZNBibnOaeDn5nl1lTdlt9W4eBnZrlUo+XXGRz8zCyXQDR2g7FSBz8zy83dXjMrnECsj7rOrsYWc/Azs1yyh5zd7TWzAvKAh5kVToRoDLf8zKyAmtzyM7OiyQY8un7o6PpXYGZblQc8zKywGv2cn5kVjd/wMLPCavJor5kVTTaxgYOfmRVMIDZ0g9fbun74NrOtKgIao0dZSzlaSVo+WtJfUnLyX0jqlcp7p+/z0/pRLY5xYSqfJ+mYcs7r4GdmOYmmMpcyNSctb/ZdYFJE7AasAE5P5acDK1L5pLQdksaS5QDZiyxf79WS2m2aOviZWS5B5Vp+myctT0mLjgDuTJvcTJbECLKk5Tenz3cCR6btjwduj4h1EbGQLMfHuPbO7Xt+ZpZbjgGPYZIea/H92oi4tsX35qTlA9L3ocAbEdGQvrdMQL4xOXlENEhambbfCXi0xTHLSlru4GdmuQTKM5lpm3l7WyYtl3R4pepXLgc/M8slS11ZkdDxrqTlwA+BQZK2Sa2/5sTk8E7S8lckbQNsByyn7WTmJfmen5nlVNWk5Z8F/gR8Mm02Afhd+jwlfSetvy+ls5wCnJhGg0cDY4AZ7V2FW35mlktQ9Tc8vg7cLunbwF+BG1L5DcDPJM0H6skCJhHxtKQ7gLlAA3BWRDS2dxIHPzPLrcpJyxfQymhtRLwNfKqN/S8DLstzTgc/M8slQn6318yKJxvw6Pqvtzn4mVlOzuFhZgWUDXh4MlMzKyBPaWVmhZPzDY+a5eBnZrk5gZGZFU4EbGhy8DOzgsm6vQ5+ZlZAlX7DozM4+G2Bpkb4j4/vx+D3rOdrNz3DT7+6G8/+ZTv6DsimIjvjyvnsutfqjdsvmNOfb52wL2f9eB7jPr4cgNcX9eKG83ejfklvEJx381yGj1zXKddTND16BFf9ag6vL+3FJWfuxbnfeY59xq1k9VvZr8WVF4xhwbP9N26/+z5vceXtj3P5uXvy0LRhnVXtTudHXcogaTzZFDV1wPURcXk1z7e1TbthR3bcbS1rV73ztPtJ//7CxsDWUlMj3P6dXdn7sBWblP/0K7vzD196mX0OW8nbq3ugrt+b6DKOP2UxL/2tL337N2wsu+F7o1sNbD16BKee9wKzHx68NatYo7pHt7dqV5Dm0P8xcCwwFjgpzbXfLdQv6cWc+wbz0ZOWlrX9PTeO4EPHLmfg0A0byxY9ty1NjWKfw1YC0KdfE723bapKfW1Tw3ZYx7jD65l25w5lbf8Pn1vMw9OG8sbynlWuWddQ4RwenaKa4XscMD8iFkTEeuB2srn2u4VbLxnNiRe9QI8esUn5L7+3Kxf93f7cesloNqzLfvj1S3rx2N1DOfKUVzfZdsmCbek7sIEf/uuefGP8ftz27VE0tTsRj1XC5y9awA1XjKZps/9rJnz1Ra6eMpszLlxAz57ZyqHbr+PQo5bzh9tGdEJNa0822ltX1lLLqhn8Ns63n7Q6r76kMyQ9Jumx5cu7Rqvnr/8zmIFDNzB639WblH/mghf53v2z+dZdj7N65Tbcdc3OANz6reZAuelxmhrFvBkDOekbC/nWXY+z7KXePPDL7bfWZRTWuMPreaO+J/Of7r9J+Y1XjuJfxx/AOf+4PwO2a+BTZ7wCwOf/fQGTvz+K6Ab3uSqh+SHncpZa1ukDHimZybUA++3XK9rZvCY899hAZk8fwuN/GsyGdT1Y+1Yd13x5DF+46nkAevYODvv0Uqb+NIv1C5/oz4/P2gOAt+p78vifBtOjLhgyYh27jF3N9rtmAxwfPKae+bMHwInLOufCCmLsAW9y8BH1fOiwmfTs3UTf/o2cf8U8rjg/+xlt2CDu+fX2/ONp2UzoY/ZexQVXzgNg4OANfOijK2hsEI/cO7TTrqGz1XqXthzVDH4dmle/K/jMBS/ymQteBOCZRwYy9ac78YWrnueNpT0ZtMMGImDWtKHsvMcaACb9edbGfX/61d34wFErOHB8PU2NsObNbXhz+TYMHNrA3Ie3Y/S+qzrlmorkpitHcdOVowDYZ9wb/ONpi7ji/D0YPHw9K17rBQSHHlXPi8/3A+DUIz+0cd9zv/McM+4fUujAV6nRXkl9gAeA3mSx6M6IuFjSTcBHgZVp03+OiDkpTeUPgeOANal8djrWBOAbaftvR8TNtKOawW8mMCbNqb+IbMrpk6t4vk539Zd3563lPYmAXfdazanf+VvJ7XvUwUnfWMjlJ+5NBIzaZxUfO7m8ARSrvH/7/jy2G7wBCRY824//uni3zq5SzarQaO864IiIWCWpJ/CQpD+mdedHxJ2bbX8sWX6OMcBBwDXAQZKGABcDB5LF5lmSpkTECkqoWvBLeTXPBqaRPeoyOSKertb5Osv7D3mT9x/yJgAX/aL9y/v8pPmbfN/nsJXsM31OVepm7XtyxiCenDEIgAsn7NPu9ldeuHu1q1TzIkRDBYJfSj7U3NXpmZZSt76OB25J+z0qaZCkEcDhwPSIqAeQNB0YD9xW6vxVfVgnIqZGxO4R8b40x76ZdQM5BjyGNQ9opuWMlseRVCdpDrCMLID9Ja26TNITkiZJ6p3K2hpELWtwdXOdPuBhZl1Lznt+bSYtB0hZ1vaXNAj4jaS9gQuBV4FeZIOhXwcmblGlW9H1H9M2s62u0o+6RMQbZPl6x0fEksisA27knUxubQ2iOmm5mVVfpZ7zkzQ8tfiQtC3wd8Cz6T4eaXT3BOCptMsU4BRlDgZWRsQSsnGFoyUNljQYODqVleRur5nlVqHn/EYAN6dXYXsAd0TEXZLukzQcEDAHODNtP5XsMZf5ZI+6nAoQEfWSLiV7wgRgYvPgRykOfmaWSwQ0VGAy04h4AvhAK+VHtLF9AGe1sW4yMDnP+R38zCy3Wn91rRwOfmaWixMYmVlhdYdJHhz8zCw3T2xgZoUT4Xt+ZlZIotGpK82siHzPz8wKx9nbzKyYIrvv19U5+JlZbh7tNbPCCQ94mFlRudtrZoXk0V4zK5wIBz8zK6ju8KhL179raWZbXUR5SymS+kiaIelxSU9L+lYqHy3pL5LmS/qFpF6pvHf6Pj+tH9XiWBem8nmSjinnGhz8zCyXQDQ19ShraUdz3t79gP2B8Wl6+u8CkyJiN2AFcHra/nRgRSqflLZD0liyvOB7kaWsvDrNDl2Sg5+Z5RZlLiWPkWktb+8RQHPC8pvJ8nhAlrf35vT5TuDIlOfjeOD2iFgXEQvJprlvTnrUJgc/M8snDXiUs5Azby/wN+CNiGhIm7TMwbsxP29avxIYivP2mtlWU/5zfrny9gJ7bnnlyuOWn5nllqPlV+bxNubtPQQYJKm5YdYyB+/G/Lxp/XbAcjqYt7fNlp+k/6JEfI+IL7d3cDPrfgJoatryR11SesoNEfFGi7y93yULgp8EbgcmAL9Lu0xJ3x9J6++LiJA0Bfi5pCuBHYExwIz2zl+q2/tYxy7JzLq1ACrznF9beXvnArdL+jbwV+CGtP0NwM8kzQfqyUZ4iYinJd0BzAUagLNSd7qkNoNfRNzc8rukvhGxJvflmVm3U4l3e0vk7V1AK6O1EfE28Kk2jnUZcFme87d7z0/SISkSP5u+7yfp6jwnMbNuphLPunSycgY8/hM4huzGIhHxOHBYNStlZrWsvMGOWn//t6xHXSLi5exZwo3a7U+bWTdW4626cpQT/F6WdCgQknoC5wDPVLdaZlazAqICo72drZxu75nAWWRPTC8mewfvrGpWysxqncpcale7Lb+IeB347Faoi5l1Fd2g21vOaO97Jf1e0muSlkn6naT3bo3KmVmNKsho78+BO8geSNwR+CVwWzUrZWY1rPkh53KWGlZO8OsbET+LiIa03Ar0qXbFzKx2VWIy085W6t3eIenjHyVdQPaeXQCfAaZuhbqZWa3qBqO9pQY8ZpEFu+ar/HyLdQFcWK1KmVltU4236spR6t3e0VuzImbWRXSBwYxylPWGh6S9gbG0uNcXEbdUq1JmVstqfzCjHO0GP0kXA4eTBb+pwLHAQ4CDn1lRdYOWXzmjvZ8EjgRejYhTgf3IZlA1s6JqKnOpYeV0e9dGRJOkBkkDyRKNjGxvJzPrpio3mWmnKqfl91hKLnId2QjwbLJppM2soBTlLSWPIY2U9CdJc1PS8nNS+SWSFkmak5bjWuzTanJySeNT2fz0aF67ynm394vp408k3Q0MTDOwmllRVeaeXwPwtYiYLWkAMEvS9LRuUkR8v+XGmyUn3xH4H0m7p9U/JssB8gowU9KUiJhb6uSlHnI+oNS6iJjdzoWZmbUpIpYAS9LntyQ9Q+l8uxuTkwMLUy6P5unu56fp75F0e9q2Y8EP+EGpepNlVa+ohU/053MjP1zpw1oVTVv8YGdXwXIYd8yqihwnx0POwyS1TIZ2bURc+67jSaPI8nn8BfgwcLakU8gSqX0tIlaQBcZHW+zWMjn55knLD2qvYqUecv5YezubWQEFeV5vK5m0HEBSf+BXwFci4k1J1wCXpjNdStYQO63jFW5dWQ85m5ltokLP+aXZ4X8F/HdE/BogIpa2WH8dcFf6Wio5ee6k5eWM9pqZbaJCo70iy8X7TERc2aJ8RIvNPgE8lT5PAU6U1FvSaN5JTj4TGCNptKReZIMiU9q7Brf8zCy/yrT8Pgx8DnhS0pxUdhFwkqT901leIE2qUio5uaSzgWlAHTA5Ip5u7+TlvN4msmns3xsREyXtArwnImbkukwz6z4qk7T8IVpP9NHmlHltJSePiKml9mtNOd3eq4FDgJPS97fInqkxswIqt8tb69NeldPtPSgiDpD0V4CIWJH61WZWVN18MtNmGyTVkRq6koZT868sm1k11XqrrhzldHuvAn4DbC/pMrLprP5vVWtlZrWtG2RvK+fd3v+WNItsWisBJ0TEM1WvmZnVpi5wP68c5Yz27gKsAX7fsiwiXqpmxcyshhUh+AF/4J1ERn2A0cA8spkVzKyA1A3u+pfT7d2n5fc028sX29jczKxLyP2GR5p7q90ZE8ysGytCt1fSuS2+9gAOABZXrUZmVtuKMuABDGjxuYHsHuCvqlMdM+sSunvwSw83D4iI87ZSfcysK+jOwU/SNhHRIMlTK5vZRqL7j/bOILu/N0fSFOCXwOrmlc0TD5pZwRTonl8fYDlZzo7m5/0CcPAzK6puHvy2TyO9T/FO0GvWDS7dzDqsG0SAUhMb1AH90zKgxefmxcwKqspJy4dImi7p+fTn4FQuSVelxORPtEyvK2lC2v55SRPKuYZSLb8lETGxnIOYWcFUN2n5PwP3RsTlki4ALgC+DhxLlrdjDFlqymuAgyQNAS4GDkw1m5WSlq8odfJSLb+uP1uhmVVeZKO95SwlDxOxJCJmp89vAc1Jy48Hbk6b3QyckD4fD9wSmUeBQSnZ0THA9IioTwFvOjC+vcso1fI7sr2dzaygqpu0fIeIWJJWvQrskD7vxLuTk+9UorykUknL69vb2cyKKcejLh1JWr5xXUSEVJ0Ha5y318zyq9BMzq0lLQeWNufuTX8uS+VtJS0vlcy8TQ5+ZpZPuYGvg0nLyRKON4/YTgB+16L8lDTqezCwMnWPpwFHSxqcRoaPTmUlOWm5meUiKvaGR1tJyy8H7pB0OvAi8Om0bipwHDCfbHb5UyG7RSfpUmBm2m5iObftHPzMLLdKBL8SScuhlQHXiAjgrDaONRmYnOf8Dn5mll83eMPDwc/M8nPwM7PCKdCsLmZmm3LwM7Mi6u6TmZqZtcrdXjMrnjLf3qh1Dn5mlp+Dn5kVTQXf8OhUDn5mlpuaun70c/Azs3x8z8/MisrdXjMrJgc/Mysit/zMrJi6QfDzTM5mlk+FsrcBSJosaZmkp1qUXSJpkaQ5aTmuxboLU97eeZKOaVE+PpXNT+ku2+XgZ2a5ND/nt6VJy5ObaD3N5KSI2D8tUwEkjQVOBPZK+1wtqU5SHfBjsry+Y4GT0rYludtrZvlFZfq9EfFASltZjuOB2yNiHbBQ0nxgXFo3PyIWAEi6PW07t9TB3PIzs9xytPyGSXqsxXJGmac4W9ITqVs8OJVtnby9Vp7hO67n/B++xKDhDRAw9dah/PaG4bx37Fq+dPkrbNuviaWv9OK7Z+3CmlV17+y303quu38et/5gB+78yfadeAXF0dgIXxq/O0NHbODSWxZy7gm7sTb9TN5Yvg177L+GS25cSARc882dmHHfQPps28TXJr3EmH3XArDslZ5MOm8kry3uhQSX3rqA94xc35mXtfXle8i53by9rbgGuDSd5VLgB8BpOY/RrqoFP0mTgb8HlkXE3tU6T2drbBDXTtyR+U/2Zdt+jfzo7ueY/cAAvvL9l7lu4o48+Wh/jj5xOZ/8wjJuuWLExv0+f/FiZt43oBNrXjy/vX44I8esY82qrMNz5W/nb1w38V9GccgxKwGYed8AFi3szY0PP8Ozs/vyXxfuzFV/eB6AK87ZlRO//Cof/Ogq1q7uQZXyade8as7nFxFLN55Hug64K30tlZ+3pvL23kTrNzK7lfplPZn/ZF8A1q6u4+X5fRg2YgM7v3cdTz7aD4C/PjCAj3x85cZ9Dhm/kldf7sWLz/XplDoX0WuLezLj3oEce/Lyd61b/VYPHn+4P4eOz35Gj0zbjqM+WY8E7//gGlavrGP50m148bneNDbABz+6CoBt+zXRp29xg18lRntbPXZKWJ58AmgeCZ4CnCipt6TRwBhgBlnKyjGSRkvqRTYoMqW981Qt+EXEA0C7uTO7kx12Xs/79l7Ls7P78uJzfThk/JsA/K+/X8nwHTcA0KdvI5/+4jJu/cEOnVnVwvnJxTvxL99YjFr5F//nu7dj/4+sot+A7Lf19Vd7bvx5AQzbcQPLX+3Jor/1od92jUw8fRRf/LvduW7ijjQ2bq0rqCFBNuBRztIOSbcBjwB7SHol5er9nqQnJT0BfAz4KkBEPA3cQTaQcTdwVkQ0RkQDcDZZovJngDvStiV1+j2/dAP0DIA+9O3k2nRcn76NfPP6F/jJf+zImlV1XHnuSL5w6SI++5WlPHLPQBrWZ+lJP3feUn5z3XDeXlPXzhGtUh6dPpBBwxoYs+9aHv9z/3etv/+3gxnfSotwc42N8NRf+nP1PfPYfqf1XHbmKKb/YgjjTy7U//FA5d7wiIiTWim+ocT2lwGXtVI+lSypedk6PfhFxLXAtQADNaRL9iHqtgm+ef0L3PfrwTz8x0EAvDy/Dxed9D4AdnrvOg46MmsF7vmBNXzk429w+jcW039gI9Ek1q/rwZQbh3Va/bu7uTP78eg9A5l571jWrxNr3qrju2fvwtd/9BIrl9cxb05fLr5h4cbth71nA68t7rnx++uLezL0PRtobBDv22stI3bNBjgOHb+SZ2d13f+wt0iX/E3dVKcHv64vOPcHL/Py83349bXDN5ZuN3QDK5f3RApOPmcpd/1sKABf+8RuG7f5p6+9yturHfiq7bSLlnDaRUsAePzP/bnzJ8P5+o9eAuDBPwzioKPepFefd36bDz76TabcOIzDT3iDZ2f3pe/ARobu0MCgYQ2serOON5bXMWhoI3Me6s/u+63tlGvqTJ7M1ADYa9xqjvrUChbM7cPV0+cBcON3RrDT6HX8739+HYCH/7gd99w+pDOraW34f78bzKfPXrpJ2bgj32TmvQM49dD30zs96gJQVwf/+s1FXPDp3YiAMfuu5djPtt9d7nYiusVkpooKPan9rgNnNzIPB4YBS4GLI6LNvjxk3d6DdGRV6mPVMW3xnM6uguUw7piXeezxt7UlxxgwaOf4wGHnlLXtg7//t1kdeM5vq6hay6+NG5lm1g2422tmxRNAN+j2OviZWX5dP/Y5+JlZfu72mlkhdYfRXgc/M8vHqSvNrIiyh5y7fvRz8DOz/Ko4pdXW4uBnZrm55WdmxeN7fmZWTN3j3V4HPzPLrxt0e529zczyqX7S8iGSpkt6Pv05OJVL0lUpMfkTkg5osc+EtP3zkiaUcxkOfmaWX4Wmsaf1XD8XAPdGxBjg3vQdsqTkY9JyBlmWNyQNAS4GDiLL43txi3SXbXLwM7P8osylvcO0nuvneODm9Plm4IQW5bdE5lFgUEp2dAwwPSLqI2IFMJ0ykqf5np+Z5aamsh/0GybpsRbfr02pK0rZISKWpM+vAs3Zvpy03Mw6UZDnIeeOJC1/51QRoSolR3a318xyEYGivKWDljbn7k1/LkvlbSUtL5XMvE0OfmaWX+UGPFozBWgesZ0A/K5F+Slp1PdgYGXqHk8DjpY0OA10HJ3KSnK318zyq9Bzfi1z/Uh6hWzU9nLgjpTA/EXg02nzqcBxwHxgDXBqVpWol3QpMDNtNzEi2k2m7OBnZvnku+dX+lBt5/p5VyazyLKtndXGcSYDk/Oc28HPzHLLMdpbsxz8zCynLbqfVzMc/Mwsn8DBz8wKquv3eh38zCw/T2ZqZsXk4GdmhRMBjV2/3+vgZ2b5ueVnZoXk4GdmhROAc3iYWfEEhO/5mVnRBB7wMLOC8j0/MyskBz8zK57uMbGBZ3I2s3wCaGoqb2mHpBckPSlpTnOio47k7e0IBz8zy6+y09h/LCL2b5HoKFfe3o5y8DOznNLrbeUsHZM3b2+HOPiZWT4BEU1lLeUdjXskzZJ0RirLm7e3QzzgYWb5lf+GR3tJyz8SEYskbQ9Ml/Rsy52rmbfXwc/M8iv/fl7JpOURsSj9uUzSb4BxpLy9EbGkzLy9HeJur5nlE1GR0V5J/SQNaP5Mlm/3KfLn7e0Qt/zMLL/KPOe3A/AbSZDFop9HxN2SZpIjb29HOfiZWU5BNDZu+VEiFgD7tVK+nJx5ezvCwc/M8vGUVmZWWJ7SysyKJoBwy8/MCic8mamZFVQlBjw6m6KGpqaR9BrZ0HZ3Mwx4vbMrYbl015/ZrhExfEsOIOlusr+fcrweEeO35HzVUlPBr7uS9Fipp9yt9vhn1v35DQ8zKyQHPzMrJAe/rePa9jexGuOfWTfne35mVkhu+ZlZITn4mVkhOfhVkaTxkualbFMXtL+HdTZJkyUtk/RUZ9fFqsvBr0ok1QE/Jss4NRY4SdLYzq2VleEmoCYfyrXKcvCrnnHA/IhYEBHrgdvJsk9ZDYuIB4D6zq6HVZ+DX/VUNNOUmVWWg5+ZFZKDX/VUNNOUmVWWg1/1zATGSBotqRdwIln2KTOrAQ5+VRIRDcDZwDTgGeCOiHi6c2tl7ZF0G/AIsIekV1IGMeuG/HqbmRWSW35mVkgOfmZWSA5+ZlZIDn5mVkgOfmZWSA5+XYikRklzJD0l6ZeS+m7BsW6S9Mn0+fpSky5IOlzSoR04xwuS3pXlq63yzbZZlfNcl0g6L28drbgc/LqWtRGxf0TsDawHzmy5UlKH8jBHxL9ExNwSmxwO5A5+ZrXMwa/rehDYLbXKHpQ0BZgrqU7SFZJmSnpC0ucBlPlRml/wf4Dtmw8k6X5JB6bP4yXNlvS4pHsljSILsl9Nrc7/JWm4pF+lc8yU9OG071BJ90h6WtL1gNq7CEm/lTQr7XPGZusmpfJ7JQ1PZe+TdHfa50FJe1biL9OKp0MtBetcqYV3LHB3KjoA2DsiFqYAsjIiPiSpN/CwpHuADwB7kM0tuAMwF5i82XGHA9cBh6VjDYmIekk/AVZFxPfTdj8HJkXEQ5J2IXuL5f3AxcBDETFR0seBct6OOC2dY1tgpqRfRcRyoB/wWER8VdJ/pGOfTZZY6MyIeF7SQcDVwBEd+Gu0gnPw61q2lTQnfX4QuIGsOzojIham8qOBfZvv5wHbAWOAw4DbIqIRWCzpvlaOfzDwQPOxIqKtee2OAsZKGxt2AyX1T+f4P2nfP0haUcY1fVnSJ9Lnkamuy4Em4Bep/Fbg1+kchwK/bHHu3mWcw+xdHPy6lrURsX/LghQEVrcsAr4UEdM22+64CtajB3BwRLzdSl3KJulwskB6SESskXQ/0KeNzSOd943N/w7MOsL3/C+Pj/IAAAD1SURBVLqfacAXJPUEkLS7pH7AA8Bn0j3BEcDHWtn3UeAwSaPTvkNS+VvAgBbb3QN8qfmLpOZg9ABwcio7FhjcTl23A1akwLcnWcuzWQ+gufV6Mll3+k1goaRPpXNI0n7tnMOsVQ5+3c/1ZPfzZqckPD8la+H/Bng+rbuFbOaSTUTEa8AZZF3Mx3mn2/l74BPNAx7Al4ED04DKXN4Zdf4WWfB8mqz7+1I7db0b2EbSM8DlZMG32WpgXLqGI4CJqfyzwOmpfk/j1ADWQZ7VxcwKyS0/MyskBz8zKyQHPzMrJAc/MyskBz8zKyQHPzMrJAc/Myuk/w9P6kKUCF1c5wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}